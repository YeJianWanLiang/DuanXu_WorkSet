warning off
clear
clc

addpath(genpath('.'));
starttime = datestr(now,0);

% load 'data/Corel16k001.mat'; % 10 5 0.1

% load 'data/Corel5k.mat'; % 10 4 0.1

% load 'data/Image.mat'; % 10 3 0.1

load 'data/scene.mat'; % 5 2 1
target = targets';

% load 'data/yeast.mat'; % 2^2 2^-4 10  k=14
% data = [X_train;X_test];
% target = [Y_train;Y_test]';

% load 'data/yeastMLKNN.mat';
% target = targets';

% load 'data/emotions.mat';
% target = targets';


%% Optimization Parameters
optmParameter.alpha   = 2^5;  % 2.^[-10:10] % label correlation
optmParameter.beta    = 2^2; % 2.^[-10:10] % sparsity 稀疏性
optmParameter.gamma   = 1; % {0.1, 1, 10} % initialization for W

optmParameter.maxIter           = 100;
optmParameter.minimumLossMargin = 0.0001;
optmParameter.bQuiet             = 1;

%% Model Parameters
modelparameter.crossvalidation    = 1; % {0,1}
modelparameter.cv_num             = 10;
modelparameter.L2Norm             = 1; % {0,1}
modelparameter.drawNumofFeatures  = 0; % {0,1}
modelparameter.deleteData         = 0; % {0,1}


if exist('train_data','var')==1
    data=[train_data;test_data];
    target=[train_target,test_target];
    clear train_data test_data train_target test_target
end
data     = double(data);
num_data = size(data,1);
if modelparameter.L2Norm == 1
    temp_data = data;
    temp_data = temp_data./repmat(sqrt(sum(temp_data.^2,2)),1,size(temp_data,2));
    if sum(sum(isnan(temp_data)))>0
        temp_data = data+eps;
        temp_data = temp_data./repmat(sqrt(sum(temp_data.^2,2)),1,size(temp_data,2));
    end
else
    temp_data = data;
end
if modelparameter.deleteData
    clear data
end

randorder = randperm(num_data);
Result_LLSF  = zeros(15,modelparameter.cv_num);

for j = 1:modelparameter.cv_num
    %     fprintf('Running Fold - %d/%d \n',j,modelparameter.cv_num);

    %% the training and test parts are generated by fixed spliting with the given random order
    [cv_train_data,cv_train_target,cv_test_data,cv_test_target ] = generateCVSet(temp_data,target',randorder,j,modelparameter.cv_num );
    cv_train_target=cv_train_target';
    cv_test_target=cv_test_target';
end

%% If we don't search the parameters, we will run LLSF with the fixed parametrs
[W]  = LS( cv_train_data, cv_train_target',optmParameter);

fprintf("类属特征计算完成\n");

data_LS = data * W;


[M,N]=size(data_LS);%数据集为一个M*N的矩阵，其中每一行代表一个样本

indices=crossvalind('Kfold',data_LS(1:M,N),10);%进行随机分包
para_k = 14;% k=14
HammingLoss = zeros(1,10);
RankingLoss = zeros(1,10);
OneError = zeros(1,10);
Coverage = zeros(1,10);
Average_Precision = zeros(1,10);
for k=1:10 %交叉验证k=10，10个包轮流作为测试集
    fprintf("The %d-Fold Cross Validation\n", k);
    test = (indices == k); %获得test集元素在数据集中对应的单元编号
    train = ~test;%train集元素的编号为非test元素的编号
    train_data=data_LS(train,:);%从数据集中划分出train样本的数据
    train_target=target(:,train);%获得样本集的测试目标，在本例中是实际分类情况
    test_data=data_LS(test,:);%test样本集
    test_target=target(:,test);
    [HammingLoss(1,k),RankingLoss(1,k),OneError(1,k),Coverage(1,k),Average_Precision(1,k),Outputs,Pre_Labels.MLKNN]=MLKNN_Algorithm(train_data,train_target,test_data,test_target,para_k,1,2);%要验证的算法

end

ResultAll=zeros(5,2);

ResultAll(1,1) = mean(HammingLoss);
ResultAll(2,1) = mean(OneError);
ResultAll(3,1) = mean(RankingLoss);
ResultAll(4,1) = mean(Coverage);
ResultAll(5,1) = mean(Average_Precision);

ResultAll(1,2) = std(HammingLoss);
ResultAll(2,2) = std(OneError);
ResultAll(3,2) = std(RankingLoss);
ResultAll(4,2) = std(Coverage);
ResultAll(5,2) = std(Average_Precision);
PrintResults(ResultAll);